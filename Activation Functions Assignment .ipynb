{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### f(x) = 1/(1+e^-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid functon is also known as 'Logistic' function. \n",
    "This is a nonlinear activation function.\n",
    "This function is similar to probability functions. \n",
    "I provides only 1 or 0.\n",
    "It provides smooth gradient. Commonly used in output layer.\n",
    "It faces vanishing gradient problem. \n",
    "Ths is non zero centric function.\n",
    "It is an exponetial function. \n",
    "Hence if we have a number of parameters it generates space and time complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='sigmoid.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TanH function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### f(x) = ((e^x)-(e^-x))/((e^x)+(e^-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is an exponential function. It is a zero centric function.\n",
    "result ranges in between -1 and +1. It also provides smooth gradient.\n",
    "It results normalised values.\n",
    "But it face vanishing gradient problem. It not used in denser models.\n",
    "Mainly used for Binary Classification. It is mainly used in hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='tanh.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ReLU (Rectified linear unit) Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### f(x)=max(0,x)     ;   if x>0\n",
    "##### f(x)=0      ;    if x<=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the postive inputs it provides the maximum of positive inputs as output.\n",
    "For negative and 0 input it generates 0 as output.\n",
    "It doesn't face gradient vanishing problem. This function fast in computation.\n",
    "It is a non zero centric function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='relu.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Softmax Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an exponential function. It is mainly used in output layer. Mainly used for Multi class classification. It gives output as the probability of occurance of the input. Here larger input correspond to larger probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='softmaxeq.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Leaky ReLU Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### for positive inputs it compute output as 'max(0,x)'\n",
    "##### for negative inputs it comput oupute as '0.01*x'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to ReLU. It only change from ReLu in the case of negative input. It have ouput for negative inputs. Hence it doesn't face any gradient vanishing problem. Only simple calculation is required. We can compute gradient for all the input. Computation of gradient is easy and not take too much time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='leakyrelu.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. ELU (Exponential Linear Unit) Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### f(x)= x                          ; if x>0\n",
    "##### f(x)=a*(e^x-1)              ; if x<=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also similar to ReLU function. This function is also free from gradient vanishing problem. Because it have outputs in both positive and negative axis. The average of the output is closer to 0. Hence this function is a zero centric function. The output function for negative input contains exponetial function. Hence to comput output for negative time takes a litle bit time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='elu.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. PReLU (Parametric ReLU) Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### F(x)=max(0,x) ; for x>0\n",
    "##### F(x)=ax   ;   for x<=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where a is the parameter, which is a learnable parameter for negative inputs. This is also similar to ReLU. We can compute gradiens. It doesn's face gradient vanishing problem.It require only simple calculation. Hence it is faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='prelu.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Swish Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### f(x)=x*sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swish function is also powerful as ReLU. It can be used in LSTM networks. In this we doesn't face gradient vanishing problem. Here we are able to get derivative all type inputs such large input, small input,positive input,negative input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='swish.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Maxout Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### f(x)=max((w1*x1+b1) , (w2*x2+b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a learnable activation function. It train weights and baises. This is a linear function. It doesn't depend on the predefined satte. this is applicable for both positive and negative inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. SoftPlus Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### f(x)=log(1+e^x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function provides smooth curve and gradient. We can find derivative of this functon. sometimes this performs better than ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='softplus.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
